{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca3dae3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53615c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f758db0",
   "metadata": {},
   "source": [
    "### Projektordner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c71bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders:\n",
      "CACHE : scs_osm_project\\cache_gpkg_new\n",
      "MASTER: scs_osm_project\\master_gpkg_new\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = \"scs_osm_project\"   # hier kommen die gpkg-Ordner hin\n",
    "CSV_PATH = \"islands_master.csv\"          # CSV-Masterdatei\n",
    "\n",
    "# Ordnernamen für die gpkg-Dateien\n",
    "CACHE_DIR  = os.path.join(PROJECT_DIR, \"cache_gpkg_new\")\n",
    "MASTER_DIR = os.path.join(PROJECT_DIR, \"master_gpkg_new\")\n",
    "\n",
    "# Erstelle die beiden Projektordner , falls sie noch nicht existieren.\n",
    "# exist_ok=True verhindert einen Fehler, wenn die Ordner bereits vorhanden sind.\n",
    "for folder in [CACHE_DIR, MASTER_DIR]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Folders:\")\n",
    "print(\"CACHE :\", CACHE_DIR)\n",
    "print(\"MASTER:\", MASTER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f930f",
   "metadata": {},
   "source": [
    "### CSV laden + korrekte bbox erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle benötigten Spalten sind vorhanden.\n",
      "Alle Bounding Boxes sind vollständig.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>island</th>\n",
       "      <th>island_group</th>\n",
       "      <th>country</th>\n",
       "      <th>north</th>\n",
       "      <th>south</th>\n",
       "      <th>west</th>\n",
       "      <th>east</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cuarteron Reef</td>\n",
       "      <td>Spratly</td>\n",
       "      <td>China</td>\n",
       "      <td>8.890</td>\n",
       "      <td>8.84</td>\n",
       "      <td>112.80</td>\n",
       "      <td>112.87</td>\n",
       "      <td>(112.8, 8.84, 112.87, 8.89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fiery Cross Reef</td>\n",
       "      <td>Spratly</td>\n",
       "      <td>China</td>\n",
       "      <td>9.580</td>\n",
       "      <td>9.53</td>\n",
       "      <td>112.86</td>\n",
       "      <td>112.92</td>\n",
       "      <td>(112.86, 9.53, 112.92, 9.58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gaven Reefs</td>\n",
       "      <td>Spratly</td>\n",
       "      <td>China</td>\n",
       "      <td>10.220</td>\n",
       "      <td>10.19</td>\n",
       "      <td>114.21</td>\n",
       "      <td>114.24</td>\n",
       "      <td>(114.21, 10.19, 114.24, 10.22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hughes Reef</td>\n",
       "      <td>Spratly</td>\n",
       "      <td>China</td>\n",
       "      <td>9.927</td>\n",
       "      <td>9.90</td>\n",
       "      <td>114.48</td>\n",
       "      <td>114.51</td>\n",
       "      <td>(114.48, 9.9, 114.51, 9.927)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Johnson Reef</td>\n",
       "      <td>Spratly</td>\n",
       "      <td>China</td>\n",
       "      <td>9.750</td>\n",
       "      <td>9.69</td>\n",
       "      <td>114.26</td>\n",
       "      <td>114.30</td>\n",
       "      <td>(114.26, 9.69, 114.3, 9.75)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            island island_group country   north  south    west    east  \\\n",
       "0   1    Cuarteron Reef      Spratly   China   8.890   8.84  112.80  112.87   \n",
       "1   2  Fiery Cross Reef      Spratly   China   9.580   9.53  112.86  112.92   \n",
       "2   3       Gaven Reefs      Spratly   China  10.220  10.19  114.21  114.24   \n",
       "3   4       Hughes Reef      Spratly   China   9.927   9.90  114.48  114.51   \n",
       "4   5      Johnson Reef      Spratly   China   9.750   9.69  114.26  114.30   \n",
       "\n",
       "                             bbox  \n",
       "0     (112.8, 8.84, 112.87, 8.89)  \n",
       "1    (112.86, 9.53, 112.92, 9.58)  \n",
       "2  (114.21, 10.19, 114.24, 10.22)  \n",
       "3    (114.48, 9.9, 114.51, 9.927)  \n",
       "4     (114.26, 9.69, 114.3, 9.75)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV-Datei laden\n",
    "df = pd.read_csv(CSV_PATH, sep=\";\")\n",
    "\n",
    "# Liste aller Spalten, die wir aus der CSV brauchen\n",
    "needed_colums = [\"id\",\"island\",\"island_group\",\"country\",\"north\",\"south\",\"west\",\"east\"]\n",
    "\n",
    "# Hier wird geprüft, ob alle benötigten Spalten in der CSV-Datei vorhanden sind, es wird True oder False geprintet\n",
    "if set(needed_colums).issubset(df.columns):\n",
    "    print(\"Alle benötigten Spalten sind vorhanden.\")\n",
    "else:\n",
    "    print(\"Mindestens eine benötigte Spalte fehlt.\")\n",
    "\n",
    "\n",
    "# Kopie erstellen nur mit den benötigten Spalten, gegeben durch die Liste \"needed\"\n",
    "df = df[needed_colums].copy()\n",
    "\n",
    "# Koordinaten-Spalten in numerische Werte umwandeln, damit die Werte nicht als String interpretiert werden\n",
    "# wir haben \"errors='coerce'\" gesetzt, damit ungültige Werte zu NaN werden und kein Fehler ausgelöst wird\n",
    "for c in [\"north\", \"south\", \"west\", \"east\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Printen, ob alle Bounding Boxes vollständig sind\n",
    "if df[[\"north\",\"south\",\"west\",\"east\"]].notna().all(axis=1).all():\n",
    "    print(\"Alle Bounding Boxes sind vollständig.\")\n",
    "else:\n",
    "    print(\"Mindestens eine Bounding Box ist unvollständig.\")\n",
    "\n",
    "\n",
    "# OSMnx erwartet die Bounding Boxen als Tupel (west, south, east, north), daher erstellen wir eine neue Spalte \"bbox\" mit diesen Tupeln in der richtigen Reihenfolge\n",
    "df[\"bbox\"] = list(zip(df[\"west\"], df[\"south\"], df[\"east\"], df[\"north\"]))\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58dbc6",
   "metadata": {},
   "source": [
    "### Overpass-Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faa246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islands to process: 70\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mit OSM-Tag-Abfragen (Keys = \"Feature-Typ\", Werte = OSM-Tags)\n",
    "QUERIES = {\n",
    "    \"runways\": {\"aeroway\": \"runway\"},\n",
    "    \"helipads\": {\"aeroway\": [\"helipad\", \"heliport\"]},\n",
    "    \"towers\": {\"man_made\": \"tower\"},\n",
    "    \"marinas\": {\"leisure\": \"marina\"},\n",
    "    \"ferry_terminals\": {\"amenity\": \"ferry_terminal\"},\n",
    "}\n",
    "\n",
    "\n",
    "# Wir haben OSMMnx und nicht ohsome benutzt, da wir nur die aktuellen OSM-Daten extrahieren möchten\n",
    "\n",
    "# ----------------------\n",
    "# OSMxn (Overpass) Einstellungen [ChatGPT 1]\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# use_cache=True: aktiviert das Caching von Abfragen, um wiederholte Anfragen zu beschleunigen\n",
    "ox.settings.use_cache = True\n",
    "\n",
    "# log_console=True: aktiviert die Protokollierung von Aktivitäten in der Konsole, hilfreich zum Debuggen\n",
    "ox.settings.log_console = True\n",
    "\n",
    "# overpass_settings: bestimmt das Ausgabeformat (in unserem Fall json) und den Timeout für Overpass-Abfragen (180 Sekunden darf der Server brauchen)\n",
    "ox.settings.overpass_settings = \"[out:json][timeout:180]\"\n",
    "\n",
    "# URL zum Overpass-Server, der die OSM-Abfragen beantwortet\n",
    "ox.settings.overpass_url = \"https://overpass.kumi.systems/api/interpreter\"\n",
    "\n",
    "\n",
    "\n",
    "# Rate-Limit/Pausen zwischen Abfragen, viele Abfragen sollen wohl vom Overpass-Server gedrosselt werden\n",
    "BASE_SLEEP = 6 # Grund-Wartezeit in Sekunden zwischen zwei Overpass-Abfragen\n",
    "JITTER = 2 # Jitter ist eine zusätzliche Wartezeit, die zufällig zwischen 0 und JITTER Sekunden variiert, um die Abfragen unregelmäßiger zu machen (regelmäßige Abfragen mag der Server nicht)\n",
    "\n",
    "\n",
    "# Funktion, welche eine zufällige Zahl zwischen 0 und JITTER generiert und diese zur BASE_SLEEP addiert, um die Wartezeit (\"sleep\", aus dem Modul \"time\") zu bestimmen\n",
    "def safe_sleep():\n",
    "    time.sleep(BASE_SLEEP + random.uniform(0, JITTER))\n",
    "\n",
    "\n",
    "# Checken, wie viele Inseln abgefragt werden\n",
    "print(\"Islands to process:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c541a",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5bd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion, welche Strings (z.B. Inselnamen) in \"saubere\" Strings umwandelt, indem alle nicht-alphanumerischen Zeichen durch Unterstriche ersetzt werden\n",
    "def slug(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in str(s)).strip(\"_\")\n",
    "\n",
    "\n",
    "# Erstellt den Pfad zur Cache-Datei (Geopackage) für eine bestimmte Insel und einen bestimmten Feature-Typ\n",
    "# Der Dateiname besteht aus dem f-string: \"Insel-ID (3-stellig, mit führenden Nullen)__Inselname (sauber)__Feature-Typ.gpkg\"\n",
    "# Wir benutzen doppelte Unterstriche __ als Trenner, damit Inselnamen mit einfachen Unterstrichen _ (Ersatz für Leerzeichen) keine Probleme machen\n",
    "# Layer-Key ist der Feature-Typ (z.B. \"runways\", \"towers\", etc.)\n",
    "def cache_path(island_id, island_name, layer_key):\n",
    "    return os.path.join(CACHE_DIR, f\"{int(island_id):03d}__{slug(island_name)}__{layer_key}.gpkg\")\n",
    "\n",
    "\n",
    "# Lädt die OSM-Features über ox.features_from_bbox innerhalb einer Bounding Box mit den angegebenen Tags und gibt ein GeoDataFrame zurück\n",
    "# OSMnx speichert die OSM-ID im Index, reset_index() wandelt den Index in normale Spalten um,\n",
    "# damit alle Informationen sauber im GeoPackage gespeichert und später einfacher weiterverarbeitet werden können\n",
    "def fetch_features_bbox(bbox, tags):\n",
    "    return ox.features_from_bbox(bbox=bbox, tags=tags).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c37b4",
   "metadata": {},
   "source": [
    "### Inseln downloaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9ab294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion lädt die passenden OSM-Daten für jeden Feature-Typ in QUERIES und speichert sie als GeoPackage im Cache-Ordner\n",
    "def download_island_to_cache(island_id, island_name, country, island_group, bbox, overwrite=False):\n",
    "    \n",
    "    # Printet Infos zur momentan abgefragten Insel\n",
    "    print(f\"\\n{int(island_id):03d} | {island_name} ({country}) bbox={bbox} \")\n",
    "\n",
    "    # Schleife geht alle Eintrage in QUERIES durch für jede Insel\n",
    "    for layer_key, tags in QUERIES.items():\n",
    "        \n",
    "        # Ausgabe-Pfad der Cache-Datei für die aktuelle Insel und den aktuellen Feature-Typ mithilfe der Hilfsfunktion cache_path\n",
    "        out_path = cache_path(island_id, island_name, layer_key)\n",
    "\n",
    "        # Wenn die Overwrite=False und die Cache-Datei bereits existiert, wird sie geladen und der Download übersprungen\n",
    "        if (not overwrite) and os.path.exists(out_path):\n",
    "            try:\n",
    "                g_cached = gpd.read_file(out_path)\n",
    "                print(f\"  {layer_key}: cached ({len(g_cached)})\")\n",
    "                safe_sleep()\n",
    "                continue\n",
    "            except Exception:\n",
    "                # falls Cache-Datei kaputt ist, wird sie neu erzeugt\n",
    "                pass\n",
    "\n",
    "        # Versuche, die OSM-Features für die aktuelle bounding box und die angegebenen Tags herunterzuladen\n",
    "        try:\n",
    "            # Startzeit merken, um die Dauer des Downloads zu messen\n",
    "            t0 = time.time()\n",
    "\n",
    "            # OSM-Features innerhalb der Bounding Box abrufen (z. B. runways, helipads, etc.)\n",
    "            gdf = fetch_features_bbox(bbox, tags)\n",
    "\n",
    "            # Vergangene Zeit seit dem Start berechnen\n",
    "            dt = time.time() - t0\n",
    "\n",
    "            # Falls keine Objekte gefunden wurden, wird nichts gespeichert und direkt mit dem nächsten Feature-Typ weitergemacht\n",
    "            if gdf is None or len(gdf) == 0:\n",
    "                print(f\"  {layer_key}: no features found (0) in {dt:.1f}s\")\n",
    "                safe_sleep()\n",
    "                continue\n",
    "\n",
    "            print(f\"  {layer_key}: downloaded {len(gdf)} in {dt:.1f}s\")\n",
    "\n",
    "            # Metadaten aus OSM für jedes Objekt hinzufügen\n",
    "            gdf[\"src_island_id\"] = int(island_id)\n",
    "            gdf[\"src_island\"] = island_name\n",
    "            gdf[\"src_country\"] = country\n",
    "            gdf[\"src_group\"] = island_group\n",
    "            gdf[\"src_layer\"] = layer_key\n",
    "            # Geodataframe als GeoPackage speichern\n",
    "            gdf.to_file(out_path, driver=\"GPKG\")\n",
    "            print(f\"  {layer_key}: saved -> {os.path.basename(out_path)}\")\n",
    "\n",
    "\n",
    "        # Wenn Fehler auftreten werden diese geprintet, aber das Script läuft weiter\n",
    "        except Exception as e:\n",
    "            print(f\"  {layer_key}: download error (skipped) -> {repr(e)}\")\n",
    "\n",
    "        # Hilfsfunktion aufrufen, um den Overpass-Server nicht zu überlasten\n",
    "        safe_sleep()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8a593",
   "metadata": {},
   "source": [
    "### RUN: alle Inseln downloaden (Cache füllen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/1] Processing: id=1 | Cuarteron Reef\n",
      "\n",
      "001 | Cuarteron Reef (China) bbox=(112.8, 8.84, 112.87, 8.89) \n",
      "  runways: download error (skipped) -> InsufficientResponseError('No matching features. Check query location, tags, and log.')\n",
      "  helipads: downloaded 3 in 0.1s\n",
      "  helipads: saved -> 001__Cuarteron_Reef__helipads.gpkg\n",
      "  towers: downloaded 5 in 0.1s\n",
      "  towers: saved -> 001__Cuarteron_Reef__towers.gpkg\n",
      "  marinas: downloaded 1 in 0.1s\n",
      "  marinas: saved -> 001__Cuarteron_Reef__marinas.gpkg\n",
      "  ferry_terminals: download error (skipped) -> InsufficientResponseError('No matching features. Check query location, tags, and log.')\n",
      "\n",
      "Done downloading selected islands.\n"
     ]
    }
   ],
   "source": [
    "# overwrite=False -> nutzt Cache, lädt nicht neu\n",
    "# overwrite=True  -> lädt alles neu\n",
    "overwrite = True\n",
    "\n",
    "\n",
    "# ---------\n",
    "# Da der Download nicht für alle Inseln erfolgreich war, mussten wir nachträglich eine Funktion integrieren, um die fehlenden Inseln erneut herunterzuladen\n",
    "# Dies geschah mit Hilfe von KI-Unterstützung (ChatGPT 3)\n",
    "# ---------\n",
    "# Downloadbereich einstellen (welche Inseln herunterladen?), Werte inklusive (ChatGPT 3)\n",
    "# END_ID = None -> bis zum Ende laufen lassen\n",
    "# Notiz an Andy: Wenn du alle Inseln laden möchtest, lade nur ein paar Inseln herunter, denn der Download aller Inseln hat mehrere Stunden gedauert\n",
    "START_ID = 1\n",
    "END_ID   = None\n",
    "\n",
    "# Sortiert die CSV-Tabell nach id, damit die Inseln in sauberer Reihenfolge abgearbeitet werden\n",
    "df_run = df.sort_values(\"id\").copy()\n",
    "\n",
    "# ab START_ID laufen lassen\n",
    "df_run = df_run[df_run[\"id\"] >= START_ID].copy()\n",
    "\n",
    "# bis END_ID laufen lassen, falls gesetzt\n",
    "if END_ID is not None:\n",
    "    df_run = df_run[df_run[\"id\"] <= END_ID].copy()\n",
    "\n",
    "# total speichert die Anzahl der Inseln, die heruntergeladen werden sollen\n",
    "total = len(df_run)\n",
    "\n",
    "# Schleife, welche Zeile Zeile durch df_run iteriert und die Inseln herunterlädt\n",
    "# enumrate wird genutzt fürs Fortschritts-Printing\n",
    "for i, row in enumerate(df_run.itertuples(index=False), start=1):\n",
    "\n",
    "    # Printet den Fortschritt\n",
    "    print(f\"\\n[{i}/{total}] Processing: id={row.id} | {row.island}\")\n",
    "\n",
    "    # Ruft die Funktion auf, um die Insel herunterzuladen\n",
    "    # Wichtig: Das Processen dauert sehr lange, wenn keine Features gefunden werden (download error)\n",
    "    # Wir hatten Download-Zeiten von ca. 2-3 Minuten pro Insel\n",
    "    download_island_to_cache(\n",
    "        island_id=row.id,\n",
    "        island_name=row.island,\n",
    "        country=row.country,\n",
    "        island_group=row.island_group,\n",
    "        bbox=row.bbox,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "print(\"\\nDone downloading selected islands.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a987f",
   "metadata": {},
   "source": [
    "### Pro Feature-Typ eine Masterdatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feee8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cache files: 3\n",
      "Master runways : no data\n",
      "No duplicates removed\n",
      "Master helipads : saved master__helipads.gpkg ( 3 )\n",
      "No duplicates removed\n",
      "Master towers : saved master__towers.gpkg ( 5 )\n",
      "No duplicates removed\n",
      "Master marinas : saved master__marinas.gpkg ( 1 )\n",
      "Master ferry_terminals : no data\n"
     ]
    }
   ],
   "source": [
    "# in Cache_gpkg werden die OSM-Features pro Insel und Feature-Typ gespeichert\n",
    "# wir möchten aber pro Feature-Typ eine Master-Datei mit allen Inseln zusammenführen\n",
    "# diese Funktion liest alle Cache-Dateien ein, fasst sie pro Feature-Typ zusammen und speichert sie im Master-Ordner\n",
    "def build_master_global():\n",
    "    \n",
    "    # os.path.join(CACHE_DIR, \"*.gpkg\") baut einen Pfad, der alle GeoPackage-Dateien im Cache-Ordner umfasst\n",
    "    # glob.glob () sucht nach allen GeoPackage-Dateien im Cache-Ordner\n",
    "    # Wir speichern in der Variable files alle .gpkgnach allen GeoPackage-Dateien (*.gpkg) gesucht\n",
    "    # sorted() sortiert die Liste der Dateien alphabetisch\n",
    "    # Ergebnis: files ist eine Liste von Dateipfaden zu den Cache-Dateien (GeoPackages)\n",
    "    files = sorted(glob.glob(os.path.join(CACHE_DIR, \"*.gpkg\")))\n",
    "    \n",
    "    #Gibt aus, wie viele Cache-Dateien gefunden wurden\n",
    "    print(\"Total cache files:\", len(files))\n",
    "    if len(files) == 0:\n",
    "        print(\"No cache files found.\")\n",
    "        return\n",
    "\n",
    "    # Wir erstellen ein leeres Dictionary, in dem wir später die Daten nach Feature-Typen (Layer-Keys) gruppieren\n",
    "    buckets = {}\n",
    "\n",
    "    # Schleife über alle Keys in QUERIES\n",
    "    for layer_key in QUERIES.keys():\n",
    "        \n",
    "        # Für jeden Layer-Key wird eine leere Liste angelegt\n",
    "        # Hier werden später die GeoDataFrames gesammelt, die zu diesem Typ gehören\n",
    "        buckets[layer_key] = []\n",
    "\n",
    "    # Schleife über alle gefundenen Cache-Dateien\n",
    "    # fp ist jeweils der filepath zu einer einer Cache-GPKG\n",
    "    for fp in files:\n",
    "\n",
    "        # os.path.basename(fp) nimmt nur den Dateinamen (ohne Ordner) -> aus .../026__X__helipads.gpkg wird 026__X__helipads.gpkg\n",
    "        # os.path.splitext(...)[0] entfernt die Endung .gpkg -> aus 026__X__helipads.gpkg wird 026__X__helipads\n",
    "        # name ist also der Basisname der Datei\n",
    "        name = os.path.splitext(os.path.basename(fp))[0]\n",
    "\n",
    "        # Teilt den Dateinamen an __ auf -> 026__Fiery_Cross_Reef__helipads wird zu: [\"026\", \"Fiery_Cross_Reef\", \"helipads\"]\n",
    "        parts = name.split(\"__\")\n",
    "\n",
    "        # Basic Sicherheitstest, um zu prüfen, ob der Dateiname das erwartete Format hat (3 Teile)\n",
    "        # Falls nicht, wird die Datei übersprungen\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "\n",
    "        # Der Layer-Key ist der letzte Teil des Dateinamens (z.B. \"helipads\", \"runways\", etc.)\n",
    "        layer_key = parts[-1]\n",
    "\n",
    "        # GeoPackage mit GeoPandas einlesen und als GeoDataFrame speichern\n",
    "        g = gpd.read_file(fp)\n",
    "\n",
    "        # Wenn die Datei leer ist, wird sie übersprungen\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        # Das gelesene GeoDataFrame g wird der passenden Liste im buckets-Dictionary hinzugefügt\n",
    "        buckets[layer_key].append(g)\n",
    "\n",
    "\n",
    "\n",
    "    # Für jeden Feature-Typ (Layer-key) alle Daten zusammenführen und in einer Msater-Datei speichern\n",
    "    for layer_key in buckets.keys():\n",
    "\n",
    "        # gdfs ist die Liste aller GeoDataFrames, die zu diesem Typ gehören\n",
    "        gdfs = buckets[layer_key]\n",
    "\n",
    "        # Hier bauen wir den Pfad zur Master-Datei für den aktuellen Layer-Key\n",
    "        out_master = os.path.join(MASTER_DIR, f\"master__{layer_key}.gpkg\")\n",
    "\n",
    "        # Wenn keine Daten für diesen Layer-Key gefunden wurden, wird eine Meldung ausgegeben und weiter zum nächsten Layer-Key gegangen\n",
    "        if len(gdfs) == 0:\n",
    "            print(\"Master\", layer_key, \": no data\")\n",
    "            continue\n",
    "\n",
    "        # pd.concat() hängt alle GeoDataFrames in gdfs untereinander zu einem großen zusammen\n",
    "        # ignore_index=True macht einen neuen Index 0,1,2,… (statt alte Indizes zu behalten)\n",
    "        # gpd.GeoDataFrame() sorgt dafür, dass das Ergebnis wieder ein GeoDataFrame ist\n",
    "        # geometry=\"geometry\" sagt, dass die Spalte geometry Geometrien enthält\n",
    "        # crs=gdfs[0].crs übernimmt das Koordinatensystem vom ersten GeoDataFrame\n",
    "        g_all = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), geometry=\"geometry\", crs=gdfs[0].crs)\n",
    "\n",
    "        # Weiterer Sicherheitsschritt: \n",
    "        # Falls sich irgendwo Duplikate (wenn ein OSM-Objekt (osmid) im gleichen Layer (src_layer mehrfach vorkommt)) eingeschlichen haben, werden diese entfernt \n",
    "        # mit der Funktion drop_duplicates()\n",
    "        # Nur wenn osmid existiert, können wir Duplikate identifizieren\n",
    "        if \"osmid\" in g_all.columns:\n",
    "            g_all = g_all.drop_duplicates(subset=[\"osmid\", \"src_layer\"])\n",
    "            print(\"Duplicates removed\")\n",
    "\n",
    "        else:\n",
    "            print(\"No duplicates removed\")\n",
    "\n",
    "        # Speichert das zusammengeführte GeoDataFrame (g_all) als Geopackage im vorher definierten Master-Ordner (out_master)\n",
    "        # driver=\"GPKG\" sagt: Dateiformat = GeoPackage\n",
    "        g_all.to_file(out_master, driver=\"GPKG\")\n",
    "\n",
    "        # Printet kurze Meldung, welcher Master-Layer gespeichert wurde, wie die Datei heißt und wie viele Objekte darin sind\n",
    "        print(\"Master\", layer_key, \": saved\", os.path.basename(out_master), \"(\", len(g_all), \")\")\n",
    "\n",
    "build_master_global()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a8f0b",
   "metadata": {},
   "source": [
    "### Alle Geometrien in Punkte umwandeln für Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc9778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umzuwandelnde Dateien: 3\n",
      " - master__helipads.gpkg\n",
      " - master__marinas.gpkg\n",
      " - master__towers.gpkg\n",
      "\n",
      "---\n",
      "INPUT : master__helipads.gpkg\n",
      "OUTPUT: master__helipads_POINTS_FOR_AGOL.gpkg\n",
      "Rows after conversion: 3\n",
      "Saved: master__helipads_POINTS_FOR_AGOL.gpkg\n",
      "\n",
      "---\n",
      "INPUT : master__marinas.gpkg\n",
      "OUTPUT: master__marinas_POINTS_FOR_AGOL.gpkg\n",
      "Rows after conversion: 1\n",
      "Saved: master__marinas_POINTS_FOR_AGOL.gpkg\n",
      "\n",
      "---\n",
      "INPUT : master__towers.gpkg\n",
      "OUTPUT: master__towers_POINTS_FOR_AGOL.gpkg\n",
      "Rows after conversion: 5\n",
      "Saved: master__towers_POINTS_FOR_AGOL.gpkg\n"
     ]
    }
   ],
   "source": [
    "#----------------------\n",
    "# Da wir in ArcGIS-Online Heatmaps aus Punktdaten erstellen wollen, müssen alle Geometrien in Punkte umgewandelt werden\n",
    "#----------------------\n",
    "\n",
    "\n",
    "# MASTER_DIR ist in unserem Script bisher ein normaler String-Pfad (kommt von os.path.join(...)),\n",
    "# z.B. \"scs_osm_project/master_gpkg_test\".\n",
    "# Für Funktionen wie .glob(...) und den \"/\"-Operator zum Pfad-Zusammenbauen brauchen wir aber ein Path-Objekt.\n",
    "# Path(MASTER_DIR) wandelt den String in ein Path-Objekt um.\n",
    "MASTER_DIR_PATH = Path(MASTER_DIR)\n",
    "\n",
    "# .glob(\"master__*.gpkg\") sucht im Ordner MASTER_DIR_PATH alle Dateien,\n",
    "# deren Name mit \"master__\" beginnt und auf \".gpkg\" endet (also alle Master-GeoPackages).\n",
    "# sorted(...) sortiert die gefundenen Dateien, damit die Reihenfolge immer gleich ist.\n",
    "# Ergebnis: master_files ist eine Liste von Path-Objekten zu den gefundenen Dateien\n",
    "master_files = sorted(MASTER_DIR_PATH.glob(\"master__*.gpkg\"))\n",
    "\n",
    "\n",
    "# Neue Liste, mit den Dateien, die wir umwandeln wollen \n",
    "clean_files = []\n",
    "\n",
    "# Wir gehen jede Datei in master_files durch\n",
    "for p in master_files:\n",
    "\n",
    "    # Dateiname der aktuellen Datei holen\n",
    "    name = p.name\n",
    "\n",
    "    # Wir filtern die Dateien heraus, die bereits als _POINTS_FOR_AGOL oder _PUBLISH_READY gespeichert wurden\n",
    "    if \"_POINTS_FOR_AGOL\" in name:\n",
    "        continue\n",
    "    if \"_PUBLISH_READY\" in name:\n",
    "        continue\n",
    "\n",
    "    # Wenn die Datei nicht übersprungen wurde, wird sie der Liste clean_files hinzugefügt\n",
    "    clean_files.append(p)\n",
    "\n",
    "\n",
    "\n",
    "# Listet auf, wieviele und welche Dateien umgewandelt werden\n",
    "print(\"Umzuwandelnde Dateien:\", len(clean_files))\n",
    "for p in clean_files:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "\n",
    "\n",
    "# Hilfsfunktion, die eine Geometrie in einen Punkt umwandelt\n",
    "def geom_to_point(geom):\n",
    "    \n",
    "    # Wenn keine Geometrie vorhanden ist, nichts zurückgeben\n",
    "    if geom is None:\n",
    "        return None\n",
    "\n",
    "    # Wenn die Geometrie bereits ein Punkt ist, ändern wir nichts\n",
    "    if geom.geom_type == \"Point\":\n",
    "        return geom\n",
    "\n",
    "    # alle anderen Geometrie-Typen in einen Punkt umwandeln\n",
    "    # Man hätte auch centroid nehmen können, aber representative_point() liegt immer innerhalb des Objekts\n",
    "    try:\n",
    "        return geom.representative_point()\n",
    "    except Exception:\n",
    "        # Fallback: Schwerpunkt\n",
    "        return geom.centroid\n",
    "\n",
    "\n",
    "# Schleife über alle Dateien, die umgewandelt werden sollen\n",
    "for input_gpkg in clean_files:\n",
    "\n",
    "    # input_gpkg ist ein Dateipfad zur aktuellen Datei\n",
    "    # .stem gibt den Dateinamen ohne Endung zurück (z.B. \"master__towers\")\n",
    "    # replace(\"master__\", \"\") entfernt den Prefix \"master__\", damit nur \"towers\" übrig bleibt\n",
    "    layer_key = input_gpkg.stem.replace(\"master__\", \"\")\n",
    "\n",
    "    # Namen der Output-Datei bauen\n",
    "    # \"/\" zum aneinanderhängen von Pfad-Teilen\n",
    "    output_gpkg = MASTER_DIR_PATH / (\"master__\" + layer_key + \"_POINTS_FOR_AGOL.gpkg\")\n",
    "\n",
    "\n",
    "    # Name des Layers im GeoPackage (Beispiel: \"towers_points\")\n",
    "    layer_name = layer_key + \"_points\"\n",
    "\n",
    "\n",
    "    # Zur Übersicht printen, welche Datei gerade verarbeitet wird\n",
    "    print(\"\\n---\")\n",
    "    print(\"INPUT :\", input_gpkg.name)\n",
    "    print(\"OUTPUT:\", output_gpkg.name)\n",
    "\n",
    "    # GeoPackage-Datei einlesen -> Ergebnis ist ein GeoDataFrame (Tabelle mit Geometrien)\n",
    "    gdf = gpd.read_file(input_gpkg)\n",
    "\n",
    "    # Zeilen entfernen, bei denen keine Geometrie vorhanden ist (NaN/None)\n",
    "    gdf = gdf[gdf.geometry.notna()]\n",
    "    gdf = gdf[~gdf.geometry.is_empty]\n",
    "\n",
    "\n",
    "    # CRS auf WGS84 setzen\n",
    "    if gdf.crs is None:\n",
    "        # Falls kein CRS bekannt ist, setzen wir es\n",
    "        gdf = gdf.set_crs(epsg=4326)\n",
    "    else:\n",
    "        # Wenn ein CRS vorhanden ist, rechnen wir die Geometrien in EPSG:4326 um\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "\n",
    "\n",
    "    # Neue Liste für die Punkt-Geometrien\n",
    "    new_geoms = []\n",
    "\n",
    "    # Geometrien in Punkte umwandeln mit for-schleife\n",
    "    for geom in gdf.geometry:\n",
    "\n",
    "        # Wir nutzen die gerade definierte Funktion geom_to_point, um die Geometrie in einen Punkt umzuwandeln und\n",
    "        # sie dann new_geoms hinzuzufügen\n",
    "        new_geoms.append(geom_to_point(geom))\n",
    "\n",
    "    # Die Geometriespalte wird durch die neuen Punkt-Geometrien ersetzt\n",
    "    gdf[\"geometry\"] = new_geoms\n",
    "\n",
    "    # Zur Sicherheit: Nach der Umwandlung nochmal prüfen und ungültige/ leere Geometrien entfernen\n",
    "    gdf = gdf[gdf.geometry.notna()]\n",
    "    gdf = gdf[~gdf.geometry.is_empty]\n",
    "\n",
    "    # Anzahl der Features nach der Umwandlung printen\n",
    "    print(\"Rows after conversion:\", len(gdf))\n",
    "\n",
    "    # GeoDataFrame als neues GeoPackage speichern\n",
    "    gdf.to_file(output_gpkg, layer=layer_name, driver=\"GPKG\")\n",
    "\n",
    "    # Bestätigung ausgeben, dass die Datei gespeichert wurde\n",
    "    print(\"Saved:\", output_gpkg.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd8ae3",
   "metadata": {},
   "source": [
    "### Doppelpunkte in Unterstriche umwandeln in Feldnamen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Dateien: 3\n",
      " - master__helipads_POINTS_FOR_AGOL.gpkg\n",
      " - master__marinas_POINTS_FOR_AGOL.gpkg\n",
      " - master__towers_POINTS_FOR_AGOL.gpkg\n",
      "\n",
      "INPUT : master__helipads_POINTS_FOR_AGOL.gpkg\n",
      "OUTPUT: master__helipads_PUBLISH_READY.gpkg\n",
      "Saved: master__helipads_PUBLISH_READY.gpkg\n",
      "\n",
      "INPUT : master__marinas_POINTS_FOR_AGOL.gpkg\n",
      "OUTPUT: master__marinas_PUBLISH_READY.gpkg\n",
      "Saved: master__marinas_PUBLISH_READY.gpkg\n",
      "\n",
      "INPUT : master__towers_POINTS_FOR_AGOL.gpkg\n",
      "OUTPUT: master__towers_PUBLISH_READY.gpkg\n",
      "Saved: master__towers_PUBLISH_READY.gpkg\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ArcGIS Online akzeptiert die im vorherigen Schritt erstellten GeoPackages nicht\n",
    "# Es kommt die Fehlermeldung \"failed\" (ohne weitere Details) und die Punkte werden im Feature-Layer nicht angezeigt\n",
    "# Die KI hat die folgenden mögliche Ursachen genannt: (ChatGPT 2)\n",
    "# -> Ungültige Geometrien, Komplexe Spalten, ungültige Feldnamen, problematische Datentypen, falsches CRS\n",
    "# Durch Trial and Error haben wir herausgefunden, dass ungültige Feldnamen (die Doppelpunkte in den Spaltennamen, die als OSM-Tags mitgeliefert werden, z.B. ref:ourairports) den Fehler verursachen\n",
    "# Lösung: Wir ersetzen die Doppelpunkte in den Spaltennamen durch Unterstriche\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Alle Dateien in MASTER_DIR_PATH finden, dem Format master__[beliebiger_Text]_POINTS_FOR_AGOL.gpkg folgen und alphabetisch sortieren\n",
    "points_files = sorted(MASTER_DIR_PATH.glob(\"master__*_POINTS_FOR_AGOL.gpkg\"))\n",
    "\n",
    "# Wir printen, wie viele und welche Dateien gefunden wurden\n",
    "print(\"Gefundene Dateien:\", len(points_files))\n",
    "for p in points_files:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "# mit einer for-schleife jede Datei durchgehen\n",
    "for input_gpkg in points_files:\n",
    "\n",
    "    # layer_key aus dem Dateinamen extrahieren\n",
    "    layer_key = input_gpkg.stem.replace(\"master__\", \"\").replace(\"_POINTS_FOR_AGOL\", \"\")\n",
    "\n",
    "    # Wir wollen eine neue Datei speichern, die auf _PUBLISH_READY.gpkg endet und nicht mehr auf _POINTS_FOR_AGOL.gpkg\n",
    "    output_gpkg = input_gpkg.with_name(input_gpkg.name.replace(\"_POINTS_FOR_AGOL.gpkg\", \"_PUBLISH_READY.gpkg\"))\n",
    "\n",
    "    # Kurz printen, welche Datei gerade verarbeitet wird\n",
    "    print(\"\\nINPUT :\", input_gpkg.name)\n",
    "    print(\"OUTPUT:\", output_gpkg.name)\n",
    "\n",
    "    # Datei einlesen\n",
    "    gdf = gpd.read_file(input_gpkg)\n",
    "\n",
    "    # Hier ersetzen wir die Doppelpunkte in den Spaltennamen durch Unterstriche\n",
    "    gdf = gdf.rename(columns=lambda c: c.replace(\":\", \"_\"))\n",
    "\n",
    "    # Speichern (Layername = layer_key wie vorher)\n",
    "    gdf.to_file(output_gpkg, layer=layer_key, driver=\"GPKG\")\n",
    "\n",
    "    print(\"Saved:\", output_gpkg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96238e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIS1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
